The aim of the project is to:

1) Put the LFRic build environment into a portable package that can be deployed
on different x86 target machines to build the LFRic executable. 
2) Produce an executable that is able to make use of local MPI libraries
and therefore local fast interconnects.
3) Produce an execuatable that be run on the native system with no run-time
dependencies on the build environemnt to minimise the conflict between
the packaged libraries and local libraries used for MPI and job control.

The LFRic build environment at https://code.metoffice.gov.uk/trac/lfric/wiki/LFRicTechnical/LFRicBuildEnvironment was used. As the initial purpose of
this project was to simplify building with the Intel compiler, gfortran
was replaced by Intel fortran throughout.

The build environment was containerised and Singularity was chosen
container system due to its widespread use on HPCs and Tier II systems.

Pre-requisits:
Intel Fortran 17 or 19 on build and run system. 
Singularity on build and run system.
Ability to run Singularity in sudo on build system.
MPICH based MPI on run machine. This includeds MPICH, Intel MPI, Cray MPT
and MVAPICH.

The solution replies on the following:

Locally installed software can be used from inside a containerised shell.
The local Intel compiler can be accessed via either using bind points
when the container shell is envoked, or by including environment-modules
in the container, bind mounting the compiler's location and then use
the 'module load' command to set up the compiler.

MPI ABI https://www.mpich.org/abi/ This provides compatibility
between MPICH used to build the a execuatble and the local MPICH
derivative used by the executable at run-time. Therefore it is possible
to build the executable using one MPICH derivative, and run it using
a different MPICH derivative.

Any statically compiled library will be included in the executable at
link time.  Therefore there are no run-time dependency on these
libraries outside the container.


To facilitate requirements 2) and 3) above, the following changes were made
to the standard LFRic build environemnt.

1) gfortran replaced by Intel fortran throughout, and any necessary changes
made to compile flags and configure option.
2) All packages configured to produce static libraries. 
3) The exception to 2) are the MPI libraries, which were built with
shared libraries to use MPI ABI. 


Build environemnt containerisation workflow:

1) Build a base build container comprising of gcc and build tools only.
2) Start a shell inside this conatiner, mounting a bind point for the location
of the local Intel compiler. Then build all the LFRic software dependencies
using a common installation directory. Tar this directory.
3) Build the final container to the tarball generated by 2) plus all python
dependies and also environment-modules.

Note: This could be done in one step, but difficulties installing
Intel compiler inside a containerised environment meant that a locally
installed Intel compiler is need to build the software.


EXTERNAL_STATIC_LIBRARIES = yaxt yaxt_c xios netcdff netcdf hdf5_hl hdf5 z :libstdc++.a


Inside container:
$ ldd gungho
	linux-vdso.so.1 =>  (0x00007ffe77af6000)
	libz.so.1 => /lib64/libz.so.1 (0x00007fbfcc793000)
	libmpifort.so.12 => /container/usr/lib/libmpifort.so.12 (0x00007fbfcc556000)
	libmpi.so.12 => /container/usr/lib/libmpi.so.12 (0x00007fbfcc0c0000)
	libm.so.6 => /lib64/libm.so.6 (0x00007fbfcbdbe000)
	libiomp5.so => /opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64/libiomp5.so (0x00007fbfcba1a000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fbfcb7fe000)
	libc.so.6 => /lib64/libc.so.6 (0x00007fbfcb430000)
	/lib64/ld-linux-x86-64.so.2 (0x00007fbfcc9a9000)
	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fbfcb21a000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00007fbfcb016000)
	librt.so.1 => /lib64/librt.so.1 (0x00007fbfcae0e000)
	libifport.so.5 => /opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64/libifport.so.5 (0x00007fbfcabdf000)
	libifcoremt.so.5 => /opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64/libifcoremt.so.5 (0x00007fbfca84f000)
	libimf.so => /opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64/libimf.so (0x00007fbfca362000)
	libintlc.so.5 => /opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64/libintlc.so.5 (0x00007fbfca0f7000)
	libsvml.so => /opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64/libsvml.so (0x00007fbfc91de000)


On target machine:

$ ldd gungho
	linux-vdso.so.1 (0x00007ffd0e98e000)
	libz.so.1 => /usr/lib/libz.so.1 (0x000014e33d8e0000)
	libmpifort.so.12 => /opt/intel/compilers_and_libraries_2017.4.196/linux/mpi/intel64/lib/libmpifort.so.12 (0x000014e33d537000)
	libmpi.so.12 => /opt/intel/compilers_and_libraries_2017.4.196/linux/mpi/intel64/lib/libmpi.so.12 (0x000014e33c80f000)
	libm.so.6 => /usr/lib/libm.so.6 (0x000014e33c6c9000)
	libiomp5.so => /opt/intel/compilers_and_libraries_2017.4.196/linux/compiler/lib/intel64/libiomp5.so (0x000014e33c325000)
	libpthread.so.0 => /usr/lib/libpthread.so.0 (0x000014e33c302000)
	libc.so.6 => /usr/lib/libc.so.6 (0x000014e33c13f000)
	/lib64/ld-linux-x86-64.so.2 => /usr/lib64/ld-linux-x86-64.so.2 (0x000014e33db28000)
	libgcc_s.so.1 => /usr/lib/libgcc_s.so.1 (0x000014e33c125000)
	libdl.so.2 => /usr/lib/libdl.so.2 (0x000014e33c120000)
	librt.so.1 => /usr/lib/librt.so.1 (0x000014e33c115000)
